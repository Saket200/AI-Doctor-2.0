# ğŸ©º AI Doctor 2.0 â€” Multimodal Health Assistant

An experimental AI-powered doctor that listens, sees, and responds â€” all in one interface. Built using speech, image, and text-based interaction powered by cutting-edge models like **Whisper**, **LLaMA Instruct**, and **Groq LLM**, all integrated via **Gradio UI**.

## ğŸš€ Features

- ğŸ™ï¸ **Voice Input** via OpenAI Whisper (Speech-to-Text)
- ğŸ–¼ï¸ **Image Input** processed using LLaMA Instruct (b64 encoded)
- ğŸ§  **LLM Response** generated via Groq's fast large language model
- ğŸ”Š **Audio Output** using gTTS (Text-to-Speech)
- ğŸ’» **Interactive UI** built with Gradio

## ğŸ§© Tech Stack

- [OpenAI Whisper](https://openai.com/research/whisper) â€” Speech Recognition  
- [LLaMA Instruct](https://ai.meta.com/llama/) â€” Image Context Handling  
- [Groq LLM](https://groq.com/) â€” Fast LLM inference  
- [gTTS](https://pypi.org/project/gTTS/) â€” Text-to-Speech  
- [Gradio](https://gradio.app/) â€” Web-based UI

## ğŸ“¦ Setup

```bash
git clone https://github.com/Saket200/AI-Doctor-2.0.git
cd ai-doctor-2.0
pip install -r requirements.txt
python app.py
